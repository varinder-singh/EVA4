{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA4S5F3(6)_SandhuV.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sITpHH9rlYbf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "  \n",
        "Target :\n",
        "```\n",
        "Reducing deafult drop out value to 0.1\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Results:\n",
        "    \n",
        "\n",
        "```\n",
        "Parameters: 9,752\n",
        "Best Train Accuracy: 97.98 (18th Epoch)\n",
        "Best Train Accuracy: 99.29 (14th Epoch)\n",
        "```\n",
        "\n",
        "\n",
        "Analysis:\n",
        "   \n",
        "\n",
        "```\n",
        "Reducing dropout value at each layer certainly helped the network to be more stable and accurate. Previoulsy due to default dropout value (0.25) we were more strict with the network and penalizing it.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-7t1Y7-hV4",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kH16rnZ7wt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky3f_Odl-7um",
        "colab_type": "text"
      },
      "source": [
        "## Data Transformations\n",
        "\n",
        "We first start with defining our data transformations. We need to think what our data is and how can we augment it to correct represent images which it might not see otherwise. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtssFUKb-jqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train Phase transformations\n",
        "train_transforms = transforms.Compose([\n",
        "                                       transforms.Resize((28, 28)),\n",
        "                                       transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                       transforms.RandomRotation((-7.0, 7.0), fill=(1,)),\n",
        "                                       transforms.RandomAffine((-7.0, 7.0)),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values. \n",
        "                                       # Note the difference between (0.1307) and (0.1307,)\n",
        "                                       ])\n",
        "\n",
        "# Test Phase transformations\n",
        "test_transforms = transforms.Compose([\n",
        "                                      #  transforms.Resize((28, 28)),\n",
        "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQciFYo2B1mO",
        "colab_type": "text"
      },
      "source": [
        "# Dataset and Creating Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4A84rlfDA23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\n",
        "test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgldp_3-Dn0c",
        "colab_type": "text"
      },
      "source": [
        "# Dataloader Arguments & Test/Train Dataloaders\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8OLDR79DrHG",
        "colab_type": "code",
        "outputId": "28a84cab-df5c-4f05-8649-008714525902",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "SEED = 1\n",
        "\n",
        "# CUDA?\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", cuda)\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# dataloader arguments - something you'll fetch these from cmdprmt\n",
        "dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
        "\n",
        "# train dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n",
        "\n",
        "# test dataloader\n",
        "test_loader = torch.utils.data.DataLoader(test, **dataloader_args)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available? True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubQL3H6RJL3h",
        "colab_type": "text"
      },
      "source": [
        "# The model\n",
        "Let's start with the model we first saw"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FXQlB9kH1ov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Experiment-1 Reducing Parameters (set the model right). Impact: Parameters gone down/ no overfit/ low accuracy\n",
        "# Experiment-2 Re-Structuring model looking at the receptive field. Impact: Parameters gone down further/ no overfit/ low accuracy\n",
        "# Experiment-3 Adding BatchNorm to bump up the accuracy . Impact: limited parameters/ Overfitting/ accuracy improved\n",
        "# Experiment-4 Adding Dropout to regularize. Impact: limited parameters/ no overfit/ test accuracy consistent\n",
        "# Experiment-5 Restructuring model with GAP at the end of the convolutions. limited parameters/ underfit / no target acc\n",
        "# Experiment-6 Reducing Dropout value to 10% as in experiment-5 it was highly penalized.\n",
        "dropout_value = 0.1\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Input Block\n",
        "        self.convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.Dropout2d(dropout_value)\n",
        "        ) #input:28x28 Output:26x26 RF:3x3\n",
        "        self.convblock2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout2d(dropout_value)\n",
        "        ) #input:26x26 Output:24x24 RF:5x5\n",
        "\n",
        "          ######################### TRANSITION BLOCK 1 ############################\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) #input:24x24 Output:12x12 RF:6x6\n",
        "\n",
        "        self.conv_1x1_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(1, 1), padding=0, bias=False),\n",
        "            \n",
        "        ) #input:13x13 Output:12x12 RF:6x6\n",
        "\n",
        "        self.convblock3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout2d(dropout_value)\n",
        "        ) #input:12x12 Output:10x10 RF:10x10\n",
        "\n",
        "        self.convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout2d(dropout_value)\n",
        "        ) #input:10x10 Output:8x8 RF:14x14\n",
        "\n",
        "        self.convblock5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout2d(dropout_value)\n",
        "        ) #input:10x10 Output:8x8 RF:18x18\n",
        "\n",
        "        self.convblock6 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout2d(dropout_value)\n",
        "        ) #input:8x8 Output:6x6 RF:22x22\n",
        "\n",
        "         ######################### TRANSITION BLOCK 2 ############################\n",
        "        self.gap = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=6)\n",
        "        ) #input:6x6 Output:1x1 RF:32x32\n",
        "\n",
        "        self.conv_1x1_2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1,1), padding=0, bias=False)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.convblock1(x)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv_1x1_1(x)\n",
        "        x = self.convblock3(x)\n",
        "        x = self.convblock4(x)\n",
        "        x = self.convblock5(x)\n",
        "        x = self.convblock6(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.conv_1x1_2(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3-vp8X9LCWo",
        "colab_type": "text"
      },
      "source": [
        "# Model Params\n",
        "Can't emphasize on how important viewing Model Summary is. \n",
        "Unfortunately, there is no in-built model visualizer, so we have to take external help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5skB97zIJQQe",
        "colab_type": "code",
        "outputId": "692dcce3-a51f-4db8-edd7-bcb8fa5108fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        }
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 8, 26, 26]              72\n",
            "              ReLU-2            [-1, 8, 26, 26]               0\n",
            "       BatchNorm2d-3            [-1, 8, 26, 26]              16\n",
            "         Dropout2d-4            [-1, 8, 26, 26]               0\n",
            "            Conv2d-5           [-1, 16, 24, 24]           1,152\n",
            "              ReLU-6           [-1, 16, 24, 24]               0\n",
            "       BatchNorm2d-7           [-1, 16, 24, 24]              32\n",
            "         Dropout2d-8           [-1, 16, 24, 24]               0\n",
            "         MaxPool2d-9           [-1, 16, 12, 12]               0\n",
            "           Conv2d-10            [-1, 8, 12, 12]             128\n",
            "           Conv2d-11           [-1, 16, 10, 10]           1,152\n",
            "             ReLU-12           [-1, 16, 10, 10]               0\n",
            "      BatchNorm2d-13           [-1, 16, 10, 10]              32\n",
            "        Dropout2d-14           [-1, 16, 10, 10]               0\n",
            "           Conv2d-15             [-1, 16, 8, 8]           2,304\n",
            "             ReLU-16             [-1, 16, 8, 8]               0\n",
            "      BatchNorm2d-17             [-1, 16, 8, 8]              32\n",
            "        Dropout2d-18             [-1, 16, 8, 8]               0\n",
            "           Conv2d-19             [-1, 16, 8, 8]           2,304\n",
            "             ReLU-20             [-1, 16, 8, 8]               0\n",
            "      BatchNorm2d-21             [-1, 16, 8, 8]              32\n",
            "        Dropout2d-22             [-1, 16, 8, 8]               0\n",
            "           Conv2d-23             [-1, 16, 6, 6]           2,304\n",
            "             ReLU-24             [-1, 16, 6, 6]               0\n",
            "      BatchNorm2d-25             [-1, 16, 6, 6]              32\n",
            "        Dropout2d-26             [-1, 16, 6, 6]               0\n",
            "        AvgPool2d-27             [-1, 16, 1, 1]               0\n",
            "           Conv2d-28             [-1, 10, 1, 1]             160\n",
            "================================================================\n",
            "Total params: 9,752\n",
            "Trainable params: 9,752\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.60\n",
            "Params size (MB): 0.04\n",
            "Estimated Total Size (MB): 0.64\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1__x_SbrL7z3",
        "colab_type": "text"
      },
      "source": [
        "# Training and Testing\n",
        "\n",
        "Looking at logs can be boring, so we'll introduce **tqdm** progressbar to get cooler logs. \n",
        "\n",
        "Let's write train and test functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbkF2nN_LYIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  pbar = tqdm(train_loader)\n",
        "  correct = 0\n",
        "  processed = 0\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    # get samples\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Init\n",
        "    optimizer.zero_grad()\n",
        "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. \n",
        "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model(data)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.nll_loss(y_pred, target)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update pbar-tqdm\n",
        "    \n",
        "    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    processed += len(data)\n",
        "\n",
        "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "    train_acc.append(100*correct/processed)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drokW8wWODKq",
        "colab_type": "text"
      },
      "source": [
        "# Let's Train and test our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMCFxeAKOB53",
        "colab_type": "code",
        "outputId": "ab07a120-7996-4d1f-cff8-99e7fe1c165e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model =  Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "EPOCHS = 20\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch)\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.24216584861278534 Batch_id=468 Accuracy=80.26: 100%|██████████| 469/469 [00:24<00:00, 24.04it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.1125, Accuracy: 9701/10000 (97.01%)\n",
            "\n",
            "EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.1323808878660202 Batch_id=468 Accuracy=94.78: 100%|██████████| 469/469 [00:24<00:00, 19.27it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0524, Accuracy: 9853/10000 (98.53%)\n",
            "\n",
            "EPOCH: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.06395155936479568 Batch_id=468 Accuracy=95.91: 100%|██████████| 469/469 [00:25<00:00, 18.74it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0441, Accuracy: 9863/10000 (98.63%)\n",
            "\n",
            "EPOCH: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.07189645618200302 Batch_id=468 Accuracy=96.38: 100%|██████████| 469/469 [00:24<00:00, 23.61it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0371, Accuracy: 9895/10000 (98.95%)\n",
            "\n",
            "EPOCH: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.034438539296388626 Batch_id=468 Accuracy=96.85: 100%|██████████| 469/469 [00:24<00:00, 18.93it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0340, Accuracy: 9891/10000 (98.91%)\n",
            "\n",
            "EPOCH: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.18040384352207184 Batch_id=468 Accuracy=97.07: 100%|██████████| 469/469 [00:24<00:00, 19.07it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0302, Accuracy: 9901/10000 (99.01%)\n",
            "\n",
            "EPOCH: 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.15311740338802338 Batch_id=468 Accuracy=97.17: 100%|██████████| 469/469 [00:24<00:00, 18.92it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0315, Accuracy: 9900/10000 (99.00%)\n",
            "\n",
            "EPOCH: 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.06512442231178284 Batch_id=468 Accuracy=97.38: 100%|██████████| 469/469 [00:24<00:00, 19.24it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0304, Accuracy: 9904/10000 (99.04%)\n",
            "\n",
            "EPOCH: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.13769567012786865 Batch_id=468 Accuracy=97.43: 100%|██████████| 469/469 [00:24<00:00, 18.96it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0261, Accuracy: 9914/10000 (99.14%)\n",
            "\n",
            "EPOCH: 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.04712674394249916 Batch_id=468 Accuracy=97.59: 100%|██████████| 469/469 [00:24<00:00, 18.79it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0275, Accuracy: 9911/10000 (99.11%)\n",
            "\n",
            "EPOCH: 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.03700484707951546 Batch_id=468 Accuracy=97.60: 100%|██████████| 469/469 [00:24<00:00, 19.06it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0261, Accuracy: 9914/10000 (99.14%)\n",
            "\n",
            "EPOCH: 11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.1017102375626564 Batch_id=468 Accuracy=97.68: 100%|██████████| 469/469 [00:24<00:00, 18.85it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0244, Accuracy: 9919/10000 (99.19%)\n",
            "\n",
            "EPOCH: 12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.0501745343208313 Batch_id=468 Accuracy=97.75: 100%|██████████| 469/469 [00:24<00:00, 24.87it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0233, Accuracy: 9926/10000 (99.26%)\n",
            "\n",
            "EPOCH: 13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.04883302375674248 Batch_id=468 Accuracy=97.81: 100%|██████████| 469/469 [00:24<00:00, 18.90it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0225, Accuracy: 9929/10000 (99.29%)\n",
            "\n",
            "EPOCH: 14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.058007895946502686 Batch_id=468 Accuracy=97.85: 100%|██████████| 469/469 [00:24<00:00, 18.93it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0239, Accuracy: 9920/10000 (99.20%)\n",
            "\n",
            "EPOCH: 15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.14769457280635834 Batch_id=468 Accuracy=97.95: 100%|██████████| 469/469 [00:24<00:00, 18.78it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0249, Accuracy: 9918/10000 (99.18%)\n",
            "\n",
            "EPOCH: 16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.08304902166128159 Batch_id=468 Accuracy=97.89: 100%|██████████| 469/469 [00:24<00:00, 19.08it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0220, Accuracy: 9926/10000 (99.26%)\n",
            "\n",
            "EPOCH: 17\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.14303959906101227 Batch_id=468 Accuracy=97.97: 100%|██████████| 469/469 [00:24<00:00, 18.82it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0236, Accuracy: 9923/10000 (99.23%)\n",
            "\n",
            "EPOCH: 18\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.042083580046892166 Batch_id=468 Accuracy=97.98: 100%|██████████| 469/469 [00:24<00:00, 18.89it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0231, Accuracy: 9929/10000 (99.29%)\n",
            "\n",
            "EPOCH: 19\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.05593480169773102 Batch_id=468 Accuracy=97.96: 100%|██████████| 469/469 [00:24<00:00, 19.14it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0218, Accuracy: 9927/10000 (99.27%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87RaqGSEOWDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fig, axs = plt.subplots(2,2,figsize=(15,10))\n",
        "# axs[0, 0].plot(train_losses)\n",
        "# axs[0, 0].set_title(\"Training Loss\")\n",
        "# axs[1, 0].plot(train_acc)\n",
        "# axs[1, 0].set_title(\"Training Accuracy\")\n",
        "# axs[0, 1].plot(test_losses)\n",
        "# axs[0, 1].set_title(\"Test Loss\")\n",
        "# axs[1, 1].plot(test_acc)\n",
        "# axs[1, 1].set_title(\"Test Accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odozjbIvY12p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}