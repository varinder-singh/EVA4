{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  train  val  wnids.txt  words.txt\n",
      "images\tval_annotations.txt\n"
     ]
    }
   ],
   "source": [
    " ! git clone https://github.com/seshuad/IMagenet\n",
    "!ls './tiny-imagenet-200/'\n",
    "\n",
    "!ls './tiny-imagenet-200/val/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting loading data\n",
      "finished loading data, in 43.25619649887085 seconds\n",
      "train data shape:  (100000, 64, 64, 3)\n",
      "train label shape:  (100000, 200)\n",
      "test data shape:  (10000, 64, 64, 3)\n",
      "test_labels.shape:  (10000, 200)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import scipy.ndimage as nd\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "path = './tiny-imagenet-200/'\n",
    "\n",
    "def get_id_dictionary():\n",
    "    id_dict = {}\n",
    "    for i, line in enumerate(open( path + 'wnids.txt', 'r')):\n",
    "        id_dict[line.replace('\\n', '')] = i\n",
    "    return id_dict\n",
    "  \n",
    "def get_class_to_id_dict():\n",
    "    id_dict = get_id_dictionary()\n",
    "    all_classes = {}\n",
    "    result = {}\n",
    "    for i, line in enumerate(open( path + 'words.txt', 'r')):\n",
    "        n_id, word = line.split('\\t')[:2]\n",
    "        all_classes[n_id] = word\n",
    "    for key, value in id_dict.items():\n",
    "        result[value] = (key, all_classes[key])      \n",
    "    return result\n",
    "\n",
    "def get_data(id_dict):\n",
    "    print('starting loading data')\n",
    "    train_data, test_data = [], []\n",
    "    train_labels, test_labels = [], []\n",
    "    t = time.time()\n",
    "    for key, value in id_dict.items():\n",
    "        train_data += [cv2.imread( path + 'train/{}/images/{}_{}.JPEG'.format(key, key, str(i)), cv2.IMREAD_COLOR) for i in range(500)]\n",
    "        train_labels_ = np.array([[0]*200]*500)\n",
    "        train_labels_[:, value] = 1\n",
    "        train_labels += train_labels_.tolist()\n",
    "\n",
    "    for line in open( path + 'val/val_annotations.txt'):\n",
    "        img_name, class_id = line.split('\\t')[:2]\n",
    "        test_data.append(cv2.imread( path + 'val/images/{}'.format(img_name) ,cv2.IMREAD_COLOR))\n",
    "        test_labels_ = np.array([[0]*200])\n",
    "        test_labels_[0, id_dict[class_id]] = 1\n",
    "        test_labels += test_labels_.tolist()\n",
    "\n",
    "    print('finished loading data, in {} seconds'.format(time.time() - t))\n",
    "    return np.array(train_data), np.array(train_labels), np.array(test_data), np.array(test_labels)\n",
    "  \n",
    "train_data, train_labels, test_data, test_labels = get_data(get_id_dictionary())\n",
    "\n",
    "print( \"train data shape: \",  train_data.shape )\n",
    "print( \"train label shape: \", train_labels.shape )\n",
    "print( \"test data shape: \",   test_data.shape )\n",
    "print( \"test_labels.shape: \", test_labels.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate Train and Validation for TinyImage-200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VStacking train data of size (100000, 64, 64, 3) with test data of size (10000, 64, 64, 3) gives (110000, 64, 64, 3)\n",
      "VStacking train label of size (100000, 64, 64, 3) with test label of size (10000, 64, 64, 3) gives (110000, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "# Add the two arrays and again split into 70 & 30 data\n",
    "\n",
    "def addNumpyAsVStack(dataSet,labelSet):\n",
    "    \"\"\"\n",
    "    This method adds two numpy arrays into one.\n",
    "    Arg1: Tuple, Train and Test data to be added\n",
    "    Arg2: Tuple, Trains and Test Labels to be added\n",
    "    Return: two Vstacked arrays for data and label\n",
    "    \"\"\"\n",
    "    train_data, test_data = dataSet\n",
    "    train_labels, test_labels = labelSet\n",
    "    \n",
    "    data = np.vstack((train_data,test_data))\n",
    "    label = np.vstack((train_labels, test_labels))\n",
    "    print(\"VStacking train data of size {} with test data of size {} gives {}\".format(train_data.shape,test_data.shape, data.shape))\n",
    "    print(\"VStacking train label of size {} with test label of size {} gives {}\".format(train_data.shape,test_data.shape, data.shape))\n",
    "    return (data, label)\n",
    "\n",
    "data, label = addNumpyAsVStack((train_data,test_data ), (train_labels,test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(data, label ):\n",
    "    \"\"\"\n",
    "    This method shuffles numpy array.\n",
    "    Arg1: Numpy Array, Data to be shuffled\n",
    "    Arg2: Numpy Array, Labels to be shuffled\n",
    "    Return: Two Shuffled numpy arrays\n",
    "    \"\"\"\n",
    "    size = len(data)\n",
    "    train_idx = np.arange(size)\n",
    "    np.random.shuffle(train_idx)\n",
    "\n",
    "    return data[train_idx], label[train_idx]\n",
    "  \n",
    "data, label = shuffle_data(data, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Train Data (77000, 64, 64, 3), Test Data(33000, 64, 64, 3), Train Label(77000, 200) and Test Label(33000, 200)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(data,label,train_size=0.7,test_size=0.3)\n",
    "\n",
    "print(\"Size of Train Data {}, Test Data{}, Train Label{} and Test Label{}\".format(x_train.shape,x_test.shape,y_train.shape,y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the numpy to speed up process in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " import numpy as np\n",
    "\n",
    " np.save('./x_train', x_train)\n",
    " np.save('./x_test', x_test)\n",
    " np.save('./y_train', y_train)\n",
    " np.save('./y_test', y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# torch.save(torch.from_numpy(x_train), './processed-data/train/x_train.pt')\n",
    "# torch.save(torch.from_numpy(x_test), './processed-data/test/x_test.pt')\n",
    "# torch.save(torch.from_numpy(y_train), './processed-data/train/y_train.pt')\n",
    "# torch.save(torch.from_numpy(y_test), './processed-data/test/y_test.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the scaled and transposed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scaledata import load_data\n",
    "import numpy as np\n",
    "\n",
    "x_train,x_test,y_train,y_test = load_data(255)\n",
    "np.save('./processed-data/train/x_train',x_train)\n",
    "np.save('./processed-data/test/x_test',x_test)\n",
    "np.save('./processed-data/train/y_train',y_train)\n",
    "np.save('./processed-data/test/y_test',y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torchvision.transforms as transforms\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision.datasets import CIFAR10\n",
    "# from lr_finder import LRFinder as lrfinder\n",
    "\n",
    "\n",
    "# optimizer = optim.SGD(mymodel.parameters(), lr=1e-7)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# lr_finder = lrfinder.LRFinder(mymodel, optimizer, criterion, device=\"cuda\")\n",
    "# lr_finder.range_test((x_test,y_test), end_lr=100, num_iter=150, step_mode=\"exp\")\n",
    "\n",
    "# lr_finder.plot(skip_end=0, accuracy_flag = True)\n",
    "# lr_finder.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
