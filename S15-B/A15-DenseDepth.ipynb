{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Calulations for depth sensing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, requires_grad=True)\n",
    "x.backward(torch.zeros(2))\n",
    "y = x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM of the inputs =======> 0.09407438337802887 with Loss as =======> 0.45296281576156616\n",
      "tensor([0.4530], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "########################## SSIM Loss ############################\n",
    "\n",
    "from loss.loss_ssim import get_ssim_loss\n",
    "img1 = torch.rand(1,1,22,22, requires_grad=True)\n",
    "img2 = torch.rand(1,1,22,22, requires_grad = True)\n",
    "\n",
    "print(get_ssim_loss(img1, img2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point Wise loss for GT and Pred images =======> 0.33211052417755127\n",
      "tensor(0.3321, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "############################# Pointwise loss #####################\n",
    "from loss.loss_point_wise import get_pointwise_loss\n",
    "\n",
    "img1 = torch.rand(1,1,22,22, requires_grad=True)\n",
    "img2 = torch.rand(1,1,22,22, requires_grad = True)\n",
    "print(get_pointwise_loss(img1, img2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss gradient for the GT and Pred =======>2.0\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "############################ Gradient Loss ########################\n",
    "\n",
    "######### Need to visit again for sum of grad along x and y axis ############\n",
    "from loss.loss_gradient import get_grad_loss\n",
    "\n",
    "img1 = torch.rand(1,1,22,22, requires_grad = True)\n",
    "img2 = torch.rand(1,1,22,22, requires_grad = True)\n",
    "\n",
    "img1.backward(torch.ones(1,1,22,22))\n",
    "img2.backward(torch.ones(1,1,22,22))\n",
    "\n",
    "print(get_grad_loss(img1, img2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point Wise loss for GT and Pred images =======> 0.3400869071483612\n",
      "Loss gradient for the GT and Pred =======>2.0\n",
      "SSIM of the inputs =======> -0.009599928744137287 with Loss as =======> 0.5047999620437622\n",
      "Loss in Dense Depth is =======> 2.538808822631836\n",
      "Loss in Dense Depth is ==========> 2.538808822631836\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from loss.loss import get_loss\n",
    "\n",
    "img1 = torch.rand(1,1,22,22, requires_grad=True)\n",
    "img2 = torch.rand(1,1,22,22, requires_grad = True)\n",
    "\n",
    "img1.backward(torch.ones(1,1,22,22))\n",
    "img2.backward(torch.ones(1,1,22,22))\n",
    "\n",
    "print(\"Loss in Dense Depth is ==========> {}\".format(get_loss(img1, img2).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/varinder/Documents/EVA4/S14-15/MYDenseDepth\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Decoder is called\n",
      "Forward of Decoder is called with input shape is torch.Size([2, 512, 28, 28])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,728\n",
      "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,864\n",
      "       BatchNorm2d-4         [-1, 64, 224, 224]             128\n",
      "            Conv2d-5         [-1, 64, 224, 224]          36,864\n",
      "       BatchNorm2d-6         [-1, 64, 224, 224]             128\n",
      "        BasicBlock-7         [-1, 64, 224, 224]               0\n",
      "            Conv2d-8         [-1, 64, 224, 224]          36,864\n",
      "       BatchNorm2d-9         [-1, 64, 224, 224]             128\n",
      "           Conv2d-10         [-1, 64, 224, 224]          36,864\n",
      "      BatchNorm2d-11         [-1, 64, 224, 224]             128\n",
      "       BasicBlock-12         [-1, 64, 224, 224]               0\n",
      "           Conv2d-13        [-1, 128, 112, 112]          73,728\n",
      "      BatchNorm2d-14        [-1, 128, 112, 112]             256\n",
      "           Conv2d-15        [-1, 128, 112, 112]         147,456\n",
      "      BatchNorm2d-16        [-1, 128, 112, 112]             256\n",
      "           Conv2d-17        [-1, 128, 112, 112]           8,192\n",
      "      BatchNorm2d-18        [-1, 128, 112, 112]             256\n",
      "       BasicBlock-19        [-1, 128, 112, 112]               0\n",
      "           Conv2d-20        [-1, 128, 112, 112]         147,456\n",
      "      BatchNorm2d-21        [-1, 128, 112, 112]             256\n",
      "           Conv2d-22        [-1, 128, 112, 112]         147,456\n",
      "      BatchNorm2d-23        [-1, 128, 112, 112]             256\n",
      "       BasicBlock-24        [-1, 128, 112, 112]               0\n",
      "           Conv2d-25          [-1, 256, 56, 56]         294,912\n",
      "      BatchNorm2d-26          [-1, 256, 56, 56]             512\n",
      "           Conv2d-27          [-1, 256, 56, 56]         589,824\n",
      "      BatchNorm2d-28          [-1, 256, 56, 56]             512\n",
      "           Conv2d-29          [-1, 256, 56, 56]          32,768\n",
      "      BatchNorm2d-30          [-1, 256, 56, 56]             512\n",
      "       BasicBlock-31          [-1, 256, 56, 56]               0\n",
      "           Conv2d-32          [-1, 256, 56, 56]         589,824\n",
      "      BatchNorm2d-33          [-1, 256, 56, 56]             512\n",
      "           Conv2d-34          [-1, 256, 56, 56]         589,824\n",
      "      BatchNorm2d-35          [-1, 256, 56, 56]             512\n",
      "       BasicBlock-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 512, 28, 28]       1,179,648\n",
      "      BatchNorm2d-38          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-39          [-1, 512, 28, 28]       2,359,296\n",
      "      BatchNorm2d-40          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-41          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-42          [-1, 512, 28, 28]           1,024\n",
      "       BasicBlock-43          [-1, 512, 28, 28]               0\n",
      "           Conv2d-44          [-1, 512, 28, 28]       2,359,296\n",
      "      BatchNorm2d-45          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-46          [-1, 512, 28, 28]       2,359,296\n",
      "      BatchNorm2d-47          [-1, 512, 28, 28]           1,024\n",
      "       BasicBlock-48          [-1, 512, 28, 28]               0\n",
      "================================================================\n",
      "Total params: 11,168,832\n",
      "Trainable params: 11,168,832\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 551.25\n",
      "Params size (MB): 42.61\n",
      "Estimated Total Size (MB): 594.43\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from model import ResNet,Decoder\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)\n",
    "resnet18 = ResNet.ResNet18().to(device)\n",
    "#decoder = Decoder.Decoder().to(device)\n",
    "summary(resnet18, input_size=(3, 224, 224))\n",
    "#summary(decoder, input_size=(512, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train, test\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(mymodel.parameters(), lr=0.0001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
    "    print(\"EPOCH: \", epoch)\n",
    "    train.train(resnet18, device, trainloader, epoch, optimizer)\n",
    "    test.test(resnet18, device, testloader, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with two images concatenated in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "img1 = Image.open('/home/varinder/Documents/EVA4/S14-15/Dataset/Dataset-S15/bg/bg1.jpg')\n",
    "imgtensor1 = transforms.ToTensor()(img1).unsqueeze_(0)\n",
    "print(imgtensor1.shape) \n",
    "img2 = Image.open('/home/varinder/Documents/EVA4/S14-15/Dataset/Dataset-S15/bg1/fg_1/overlay/1.jpg')\n",
    "imgtensor2 = transforms.ToTensor()(img2).unsqueeze_(0)\n",
    "print(imgtensor2.shape) \n",
    "\n",
    "imgtensor3 = torch.cat((imgtensor1,imgtensor2),1)\n",
    "print(imgtensor3.shape)\n",
    "img3 = transforms.ToPILImage()(imgtensor3.squeeze_(0))\n",
    "print(img3.getbands())\n",
    "img3.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing how Upsampling works in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 448, 448])\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "img1 = Image.open('/home/varinder/Documents/EVA4/S14-15/Dataset/Dataset-S15/bg1/fg_1/overlay/1.jpg')\n",
    "imgtensor1 = transforms.ToTensor()(img1).unsqueeze_(0)\n",
    "print(imgtensor1.shape) \n",
    "\n",
    "m = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "img_upsampled = m(imgtensor1)\n",
    "print(img_upsampled.shape)\n",
    "img_up_pil = transforms.ToPILImage()(img_upsampled.squeeze_(0))\n",
    "img_up_pil.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 448, 448])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "img1 = Image.open('/home/varinder/Documents/EVA4/S14-15/Dataset/Dataset-S15/bg1/fg_1/overlay/1.jpg')\n",
    "imgtensor1 = transforms.ToTensor()(img1).unsqueeze_(0)\n",
    "print(imgtensor1.shape) \n",
    "\n",
    "img_upsampled = F.interpolate(imgtensor1, imgtensor1.shape[-1]*2 , mode='bilinear')\n",
    "print(img_upsampled.shape)\n",
    "img_up_pil = transforms.ToPILImage()(img_upsampled.squeeze_(0))\n",
    "#img_up_pil.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
